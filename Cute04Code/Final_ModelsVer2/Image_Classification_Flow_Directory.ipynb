{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification using Flow from Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/nfsroot/data/home/2237B46'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import numpy for array operations\n",
    "import numpy as np\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define train and validation generators\n",
    "Loading images for train and test using ImageDataGenerator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6634 images belonging to 22 classes.\n",
      "Found 2847 images belonging to 22 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "batch_size = 32\n",
    "\n",
    "# Train Data Generator\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Test Data generator\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of '/train'\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'train/',  # this is the target directory\n",
    "        target_size=(224, 224),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for test data\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        'test/',\n",
    "        target_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib64/python2.7/site-packages/keras/backend/tensorflow_backend.py:1205: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /usr/lib64/python2.7/site-packages/keras/backend/tensorflow_backend.py:2755: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /usr/lib64/python2.7/site-packages/keras/backend/tensorflow_backend.py:1290: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(22, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "207/207 [==============================] - 525s - loss: 2.3749 - acc: 0.4908 - val_loss: 1.5473 - val_acc: 0.6067\n",
      "Epoch 2/4\n",
      "207/207 [==============================] - 492s - loss: 1.7109 - acc: 0.5906 - val_loss: 1.4917 - val_acc: 0.6110\n",
      "Epoch 3/4\n",
      "207/207 [==============================] - 502s - loss: 1.5092 - acc: 0.6104 - val_loss: 1.5247 - val_acc: 0.6277\n",
      "Epoch 4/4\n",
      "207/207 [==============================] - 493s - loss: 1.3618 - acc: 0.6382 - val_loss: 1.4299 - val_acc: 0.6274\n"
     ]
    }
   ],
   "source": [
    "# fit_generator is similar to 'fit'. But instead of x_train and y_train, we pass 'train_generator' \n",
    "# which already has the samples and their corresponding target information.\n",
    "\n",
    "# 'step' here is one mini-batch\n",
    "# 'steps_per_epoch' is number of batches per epoch.\n",
    "# Typically 'steps_per_epoch' should be total_samples divided by batch_size\n",
    "hist = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=6634//batch_size, # '//' in python returns only the quotient\n",
    "        epochs=4,\n",
    "        validation_data=test_generator,\n",
    "        validation_steps=2847//batch_size)\n",
    "model.save_weights('first_try.h5')  # always save your weights after training or during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'c0': 0, 'c1': 1, 'c2': 2, 'c3': 3, 'c4': 4}\n"
     ]
    }
   ],
   "source": [
    "# print(train_generator.filenames)\n",
    "print(train_generator.class_indices) # This returns class labels of directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 43 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "        'data/test/',  # this is the target directory\n",
    "        target_size=(150, 150),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')  # since we use binary_crossentropy loss, we need binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 2 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1 2 1 1 1 1\n",
      " 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "test_prob = model.predict_generator(test_generator, steps=2) # this returns the probabilities\n",
    "test_pred_classes = np.argmax(test_prob, axis=1) # convert probabilities to classes\n",
    "print(test_pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test/img_100312.jpg', 'test/img_91724.jpg', 'test/img_91729.jpg', 'test/img_91761.jpg', 'test/img_91769.jpg']\n"
     ]
    }
   ],
   "source": [
    "# Check the corresponding filenames of the predictions\n",
    "print(test_generator.filenames[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "test_predictions = pd.DataFrame(data=np.vstack((test_generator.filenames, test_pred_classes)).T, \n",
    "                                columns=['file', 'label'])\n",
    "\n",
    "test_predictions.to_csv('test_submission.csv', header=True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
